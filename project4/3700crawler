#!/usr/bin/python3 -u
import argparse
import collections
import logging
import select
import socket
import re
import time
from urllib.parse import urlparse
import weakref

# ----------------------------- GLOBAL VARIABLES -----------------------------


# Root page for Fakebook
ROOT = 'http://fring.ccs.neu.edu/fakebook/'
# Log-in form for Fakebook
LOGIN_FORM = 'http://fring.ccs.neu.edu/accounts/login/'
# Carriage return, line feed
CRLF = '\r\n'
# Maximum amount of data to be received at once from the socket
MAX_BYTES = 5000


# ------------------------- END OF GLOBALS VARIABLES -------------------------

class HTTPAddress:
    def __init__(self, url):
        self.url = url.strip()
        url = urlparse(self.url)
        self.host = url.netloc
        self.path = '/' if not url.path else url.path
        if url.query:
            self.path += '?' + url.query


class HTTPRequest:
    def __init__(self, method, http_address, headers=None, body=''):
        self.method = method
        self.http_address = http_address
        self.headers = headers if headers else {}
        self.headers.update({'Host': http_address.host})
        self.body = body

    @property
    def body(self):
        # If tried to access before body is set, return a default string
        return getattr(self, '_body', '')

    @body.setter
    def body(self, value):
        self._body = value
        self.headers['Content-Length'] = str(len(value))

    def dumps(self):
        """Formats an HTTP request"""
        msg = '{0} {1} HTTP/1.0'.format(self.method, self.http_address.path) + CRLF
        msg += CRLF.join([': '.join((key, value))
                            for key, value in self.headers.items()])
        msg += CRLF * 2
        if (self.body):
            msg += self.body + CRLF
        return msg

    def __str__(self):
        return ' '.join((self.method, self.http_address.path))


class GETRequest(HTTPRequest):
    def __init__(self, http_address, *, headers = None):
        HTTPRequest.__init__(self, 'GET', http_address, headers)


class POSTRequest(HTTPRequest):
    def __init__(self, http_address, *, headers = None, body = ''):
        HTTPRequest.__init__(self, 'POST', http_address, headers, body)

    @classmethod
    def from_form(cls, http_address, form, *, headers = None):
        """Formats the query string of a POST request"""
        request = cls(http_address, headers=headers)
        request.headers.update({
            'Content-Type': 'application/x-www-form-urlencoded'
        })
        sanatize = lambda s: s.strip().replace(' ', '+')
        request.body = '&'.join(['='.join((sanatize(key), sanatize(value)))
                                for key, value in form.items()])
        return request


class HTTPResponse:
    def __init__(self):
        self.code = 600
        self.message = 'Not a Message'
        self.headers = {}
        self.cookies = {}
        self.body = ''

    def _set_headers(self, data):
        for header in data.split(CRLF):
            parts = header.split(':', 1)
            if len(parts) < 1:
                logging.warning('header "{0}" doesnt have a ":" delimeter')
                continue
            name, value = parts

            if  re.search('[Ss]et-[Cc]ookie', name):
                match = re.search("(\w+)=(\w+)", value)
                self.cookies[match.group(1)] = match.group(2)
            else:
                self.headers[name] = value

    @classmethod
    def from_response(cls, data):
        my_response = cls()

        # Header and body deliminated with CRLFCRLF, split on first n occurences
        blocks = data.split(CRLF * 2, 1)
        raw_headers = blocks[0]

        # First line is status, not header
        header_blocks = raw_headers.split(CRLF, 1)
        raw_status = header_blocks[0]

        # Add the status code and message
        status = re.search('HTTP/1.1\W+(\d{3})\W([\ \w]+)', raw_status)
        if not status:
            logging.warning('no match in "{0}"'.format(raw_status))
        else:
            my_response.code = int(status.group(1))
            my_response.message = status.group(2)

        if len(header_blocks) > 1:
             my_response._set_headers(header_blocks[1])
        else:
            logging.warning('Recieved {0} with no headers'.format(my_response.code))

        if len(blocks) > 1:
            my_response.body = blocks[1]
        elif my_response.code < 300 or my_response.code >= 400:
            logging.warning('Recieved {0} with no body'.format(my_response.code))

        return my_response


class HTTPSession:
    def __init__(self):
        self.sock = None
        self.count = 0

    def connect(self, host):
        """Creates and connects a socket"""
        # Set up the socket
        logging.info('Opening HTTP socket to host ' + host)
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.connect((host, 80))

    def send(self, http_request):
        if self.sock is None:
            self.connect(http_request.http_address.host)
        else:
            self.count += 1
        self.sock.send(http_request.dumps().encode())

    def has_response(self):
        if self.sock is None:
            return False
        poll = select.poll()
        poll.register(self.sock.fileno(), select.POLLIN)
        events = [event for fd, event in poll.poll(0)]

        return select.POLLIN in events

    def get_response(self):
        """Returns the data retrieved from the socket"""
        data = ''
        try:
            # While we can read data, append it to the data string
            while True:
                new_data = self.sock.recv(MAX_BYTES).decode()
                data += new_data
                if not (self.has_response() and new_data): #new data '' when EOF
                    break
        except socket.timeout:
            logging.warning('Socket timed out')
            pass

        resp = HTTPResponse.from_response(data)
        if not re.search('[Kk]eep[-_\ ][Aa]live', resp.headers.get('Connection', '')):
            logging.info('Tearing down connection after {0} sent packets on a {1}'.format(self.count, resp.code))
            self.sock.close()
            self.sock = None
            self.count = 0

        return resp


class CookieMixin:
    @property
    def cookie(self):
        """Stringify the cookie dictionary"""
        if not hasattr(self, 'cookies'):
            setattr(self, 'cookies', {})
        return '; '.join(['='.join((key, value))
                                for key, value in self.cookies.items()])

class HTTPManager(CookieMixin):
    def __init__(self):
        self.http_session = HTTPSession()
        self.cookies = {}

    def request(self, http_request):
        """Makes a HTTP request"""
        http_request.headers['Connection'] = 'keep-alive'
        if self.cookie:
            http_request.headers['Cookie'] = self.cookie

        self.http_session.send(http_request)
        http_response = self.http_session.get_response()

        self.cookies.update(http_response.cookies)
        return http_response

    def request_sync(self, http_request):
        return self.request(http_request)


class HydraPromise:
    def __init__(self, http_request, session, hydra_manager):
        self.session = session
        self.http_request = http_request
        self.manager = weakref.ref(hydra_manager)

    def is_ready(self):
        return self.session and self.session.has_response()

    def resolve(self):
        if self.session is None:
            return None
        logging.debug('Resolving Promise({0})'.format(self.http_request))
        response = self.session.get_response()

        manager = self.manager()
        if manager is not None:
            manager._sessions.append(self.session)
            manager.cookies.update(response.cookies)
            logging.debug('Promise({0}) handing session back to manager'.format(self.http_request))
        self.session = None

        return response


class HydraManager(CookieMixin):
    def __init__(self):
        self._sessions = []
        self.cookies = {}

    def request(self, http_request):
        """Makes a HTTP request"""
        http_request.headers['Connection'] = 'keep-alive'
        if self.cookie:
            http_request.headers['Cookie'] = self.cookie

        if self._sessions:
            http_session = self._sessions.pop()
            logging.debug('Handing session to {0}'.format(http_request))
        else:
            http_session = HTTPSession()
            logging.debug('Creating new session for {0}'.format(http_request))
        http_session.send(http_request)
        return HydraPromise(http_request, http_session, self)

    def request_sync(self, http_request):
        return self.request(http_request).resolve()


class CrawlMixin:
    @staticmethod
    def get_linkset(data):
        """Returns the set of Fakebook links on the page"""
        matches = re.findall('href="/fakebook/([/\w]+)', data)
        return set(matches)

    @staticmethod
    def hunt_keys(data):
        """Searches the page for secret keys"""
        flags = re.findall('<h2 class=\'secret_flag\'.+>FLAG: ([a-zA-Z0-9]{64})',
            data)
        return flags

    def get_login_form(self):
        """Gets the log-in form and sets the cookie"""
        # Send a GET request for login form
        login_form = self.http_manager.request_sync(
                            GETRequest(HTTPAddress(LOGIN_FORM)))

        # Exit if the GET request was unsuccessful
        if login_form.code != 200:
            print('Unable to get log-in form')
            exit(1)
        return login_form


    def login(self):
        """Logs into Fakebook"""
        # Get the log-in form
        self.get_login_form()

        # Log in using the given credentials

        login_resp = self.http_manager.request_sync(
            POSTRequest.from_form(HTTPAddress(LOGIN_FORM), {
                'username': self.username,
                'password': self.password,
                'csrfmiddlewaretoken': self.http_manager.cookies['csrftoken'],
                'next': r'%2Ffakebook%2F'
        }))

        # Return the next page after logging in
        return self.traverse_to(HTTPAddress(login_resp.headers['Location']))

    def traverse_to(self, http_address):
        """Traverses to the given http address"""
        # Get and parse the page
        page = self.http_manager.request_sync(GETRequest(http_address))

        # Handle codes 403 (Forbidden) and 404 (Not Found)
        if page.code in (403, 404):
            return None

        # Handle codes 301 (Moved Permanently), 302 (Found),
        # and 500 (Internal Server Error)
        new_http_address = http_address
        while page.code in (301, 302, 500):
            # Get the next page if applicable
            if page.code in (301, 302):
                new_http_address = HTTPAddress(page['Location'])
            page = self.http_manager.request_sync(GETRequest(new_http_address))
        return page


class WebCrawler(CrawlMixin):
    def __init__(self, username, password):
        self.username = username
        self.password = password
        self.http_manager = HTTPManager()

    def crawl(self):
        """Runs the web crawler"""
        visited = set() # Set of visited pages
        unvisited = set() # Set of univisited pages
        secret_keys = [] # List of secret keys

        # Get the home page after logging in
        login_page = self.login()

        # Get the set of unvisited links on the home page
        unvisited = self.get_linkset(login_page.body)

        # Loop until all of the secret keys have been found
        while len(unvisited) > 0 and len(secret_keys) != 5:
            url = unvisited.pop()
            visited.add(url)

            # Go to the next unvisited page
            page = crawler.traverse_to(HTTPAddress(ROOT + url))

            # Skip if the page does not exist
            if page is None:
                continue

            # Add any secret keys if found
            secret_keys += self.hunt_keys(page.body)

            # Update unvisited
            unvisited |= (self.get_linkset(page.body) - visited)
        return secret_keys


class HydraCrawler(CrawlMixin):
    def __init__(self, username, password):
        self.username = username
        self.password = password
        self.http_manager = HydraManager()

    def crawl(self):
        """Runs the web crawler"""
        visited = set() # Set of visited pages
        unvisited = set() # Set of univisited pages
        secret_keys = [] # List of secret keys
        promises = set()

        # Get the home page after logging in
        login_page = self.login()

        # Get the set of unvisited links on the home page
        unvisited = self.get_linkset(login_page.body)

        # Loop until all of the secret keys have been found
        while (unvisited or promises) and len(secret_keys) != 5:
            ready = set(filter(lambda p: p.is_ready(), promises))
            promises -= ready
            if not ready and unvisited:
                url = unvisited.pop()
                visited.add(url)
                promises.add(self.http_manager.request(GETRequest(HTTPAddress(ROOT + url))))

            for promise in ready:
                http_response = promise.resolve()

                if http_response.code in (403, 404):
                    continue
                elif http_response.code in (301, 302):
                    promises.add(self.http_manager.request(GETRequest(HTTPAddress(page['Location']))))
                elif http_response.code in (500,):
                    promises.add(self.http_manager.request(GETRequest(promise.http_request.http_address)))
                elif http_response.code in (200,):
                    # Add any secret keys if found
                    secret_keys += self.hunt_keys(http_response.body)
                    # Update unvisited
                    unvisited |= (self.get_linkset(http_response.body) - visited)
        return secret_keys


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Fakebook login.',
                                     add_help=False)
    parser.add_argument('username')
    parser.add_argument('password')
    args = parser.parse_args()

    # Create a WebCrawler with the given username and password
    crawler = HydraCrawler(args.username, args.password)

    logging.getLogger().setLevel(logging.ERROR)

    # Start crawling
    start_time = time.time()
    keys = crawler.crawl()
    # print('Runtime {0:.2f} minutes'.format((time.time() - start_time)/60.0))
    for key in keys:
        print(key)
