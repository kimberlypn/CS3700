#!/usr/bin/python3 -u
import argparse
from urllib.parse import urlparse
import socket
import re

# ----------------------------- GLOBAL VARIABLES -----------------------------

# Root page for Fakebook
ROOT = 'http://fring.ccs.neu.edu/fakebook/'
# Log-in form for Fakebook
LOGIN_FORM = 'http://fring.ccs.neu.edu/accounts/login/'
# Carriage return, line feed
CRLF = '\r\n'
# Maximum amount of data to be received at once from the socket
MAX_BYTES = 5000

# ------------------------- END OF GLOBALS VARIABLES -------------------------

def make_http(type, path, headers={}, body=''):
    """Formats an HTTP request"""
    msg = '{0} {1} HTTP/1.1'.format(type, path) + CRLF
    for key, value in headers.items():
        if value:
            msg += (': '.join((key, value)) + CRLF)
    msg += CRLF
    msg += body + CRLF
    return msg


def format_query(form):
    """Formats the query string of a POST request"""
    c = lambda s: s.strip().replace(' ', '+')
    return '&'.join(['='.join((c(key), c(value))) for key, value in form.items()])


def get_host_path(url):
    """Returns the host and the path"""
    url = urlparse(url)
    host = url.netloc
    path = '/' if not url.path else url.path
    if url.query:
        path += '?' + url.query
    return (host, path)


class WebCrawler:
    def __init__(self, username, password):
        self.username = username # Username given by the user
        self.password = password # Password given by the user
        self.csrf = None # Initialize CSRF token to None
        self.session = None # Initialize session ID to None

    @property
    def cookie(self):
        """Generates a cookie using the CSRF token and the session ID"""
        cookie = ''
        csrf_token = 'csrftoken=' + self.csrf if self.csrf else None
        session_id = 'sessionid=' + self.session if self.session else None
        cookie = '; '.join(filter(None, [csrf_token, session_id]))
        return cookie

    def connect(self, host):
        """Creates and connects a socket"""
        # Set up the socket
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(0.3)
        # Connect to the server on the HTTP port
        sock.connect((host, 80))
        # Return the socket
        return sock

    def get_response(self, sock):
        """Returns the data retrieved from the socket"""
        data = ''
        while True:
            try:
                new_data = sock.recv(MAX_BYTES)
                # Exit the loop if all of the data has been retrieved
                if not new_data:
                    break
                # Else, append the retrieved data
                else:
                    data += new_data.rstrip().decode()
            except:
                break
        # Close the socket
        sock.close()
        # Return the data
        return data

    def parse_response(self, data):
        """Returns a dict of the headers and HTML"""
        response = {}
        data = data.split(CRLF * 2, 1)
        # Get the headers
        headers = data[0]

        # Add the status code and message
        status = re.search('(HTTP/1.1)\W(\d{3})\W(.+)', headers)
        response['Status Code'] = int(status.group(2)) if status else None
        response['Status Message'] = status.group(3) if status else None

        # Add the remaining headers; ignore the cookies since these
        # are already saved as fields
        headers = headers.split(CRLF)[1:]
        for h in headers:
            header = h.split(': ')
            if header[0] == 'Set-Cookie':
                continue
            else:
                response[header[0]] = header[1]

        # Add the HTML
        response['HTML'] = data[1] if len(data) > 1 else None

        # Return the dict
        return response

    def GET(self, url):
        """Implements the HTTP GET method"""
        # Parse the URL and retrieve the necessary fields
        host, path = get_host_path(url)

        # Send the request
        sock = self.connect(host)
        headers = {
            'Host': host,
            'Origin': ROOT,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Referer': LOGIN_FORM + '?next=/fakebook/',
            'Cookie': self.cookie
        }
        request = make_http('GET', path, headers)
        sock.send(str.encode(request))

        # Get and return the response
        resp = self.get_response(sock)
        return resp

    def POST(self, referer, url, body):
        """Implements the HTTP POST method"""
        # Parse the URL and retrieve the necessary fields
        host, path = get_host_path(url)

        # Send the request
        sock = self.connect(host)
        headers = {
            'Host': host,
            'Content-Length': str(len(body)),
            'Content-Type': 'application/x-www-form-urlencoded',
            'Origin': 'http://' + get_host_path(referer)[0],
            'Referer': referer.split('?')[0],
            'Cookie': self.cookie
        }
        request = make_http('POST', path, headers, body)
        sock.send(str.encode(request))

        # Get and return the response
        resp = self.get_response(sock)
        return resp

    def check_cookie(self, pattern, data, help_info):
        """Returns the CSRF token or session ID if found; else exits"""
        match = re.search(pattern, data)
        if not match:
            print('Couldn\'t extract ' + help_info + '. Aborting.')
            exit(1)
        else:
            return match

    def get_login_form(self):
        """Gets the log-in form and sets the cookie"""
        # Send a GET request
        login_form = self.GET(LOGIN_FORM)

        # Set the CSRF token and session ID if found
        self.csrf = self.check_cookie('[Ss]et-[Cc]ookie:\W*csrftoken=(\w+);',
            login_form, 'CSRF token').group(1)
        self.session = self.check_cookie('[Ss]et-[Cc]ookie:\W*sessionid=(\w+);',
            login_form, 'session ID').group(1)

        # Parse the log-in form
        login_form = self.parse_response(login_form)

        # Exit if the GET request was unsuccessful
        if login_form['Status Code'] != 200:
            print('Unable to get log-in form')
            exit(1)
        return login_form

    def traverse_to(self, url):
        """Traverses to the given url"""
        # Get and parse the page
        page = self.GET(url)
        page = self.parse_response(page)
        print(page)

        # Handle codes 403 (Forbidden) and 404 (Not Found)
        if page['Status Code'] in (403, 404):
            return None

        new_url = url
        while page['Status Code'] in (301, 302, 500):
            if page['Status Code'] in (301, 302):
                print(page)
                new_url = page['Location']
            page = self.GET(new_url)
            page = self_parse_response(page)
        return page

    def login(self):
        """Logs into Fakebook"""
        # Get the log-in form
        self.get_login_form()

        # Log in using the given credentials
        query = {
            'username': self.username,
            'password': self.password,
            'csrfmiddlewaretoken': self.csrf,
            'submit': 'Log in',
            'next': r'%2Ffakebook%2F'
        }
        body = format_query(query)
        login_resp = self.POST(LOGIN_FORM, LOGIN_FORM, body)

        # Set the session ID if found
        self.session = self.check_cookie('[Ss]et-[Cc]ookie:\W*sessionid=(\w+);',
            login_resp, 'session ID').group(1)

        # Parse the POST response
        login_resp = self.parse_response(login_resp)

        # Exit if the WebCrawler was unable to log in
        if login_resp['Status Code'] != 302:
            print('Unable to log in')
            exit(1)

        return self.traverse_to(login_resp['Location'])

def get_linkset(origin, page):
    matches = re.findall('href=[\'"]([/\w]+)', page['HTML'])
    for match in matches:
        print(match)
    return set()

def hunt_key(page):
    return None

def crawl(crawler):
    """Runs the web crawler"""
    visited = set()
    unvisited = set()
    secret_keys = []

    login_page = crawler.login()
    print(login_page)
    unvisited = get_linkset(ROOT, login_page)

    while len(unvisited) is not 0:
        url = unvisited.pop()
        visited.add(url)

        page = crawler.traverse_to(url)
        if page is None:
            continue
        secret_key = hunt_key(page)
        if secret_key:
            secret_keys.append(secret_key)

        unvisited |= (get_linkset(ROOT, page) - visited)
    print(visited)
    return secret_keys


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Fakebook login.',
                                     add_help=False)
    parser.add_argument('username')
    parser.add_argument('password')
    args = parser.parse_args()

    # Create a WebCrawler with the given username and password
    crawler = WebCrawler(args.username, args.password)

    # Start crawling
    crawl(crawler)
